{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import re\n",
    "import requests\n",
    "import os\n",
    "from datetime import datetime\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Usefull functions\n",
    "\n",
    "def write_list(l,file_path, header= True):\n",
    "    f = open(file_path,\"w+\")\n",
    "    initial_pos = 0\n",
    "    \n",
    "    #header\n",
    "    if header:\n",
    "        initial_pos = 1\n",
    "        str_header = ''\n",
    "        for k_header in l[0].keys():\n",
    "            str_header = str_header + str(k_header) + \",\"\n",
    "        f.write(str_header[:-1]+\"\\n\")\n",
    "        \n",
    "    #content\n",
    "    for l_index in range(initial_pos,len(l)):\n",
    "        str_row = ''\n",
    "        for k_att in l[l_index]:\n",
    "            str_row = str_row + '\"'+str(l[l_index][k_att]) +'\"'+','\n",
    "        f.write(str_row[:-1]+\"\\n\")\n",
    "        \n",
    "\n",
    "def coci_call(operation, list_dois, fields):\n",
    "    items_dict = {}\n",
    "    for doi in list_dois:\n",
    "        r = requests.get('https://opencitations.net/index/coci/api/v1/'+str(operation)+\"/\"+str(doi))\n",
    "        if len(r.json()) > 0: \n",
    "            if fields == \"*\":\n",
    "                items_dict[doi] = r.json()[0]\n",
    "            else:\n",
    "                items_dict[doi] = {}\n",
    "                for f in fields:\n",
    "                    items_dict[doi][f] = None\n",
    "                    if f in r.json()[0]:\n",
    "                        items_dict[doi][f] = r.json()[0][f]\n",
    "    return items_dict\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_1998_2004 = \"csv/1998_2004.csv\"\n",
    "csv_2005_2010 = \"csv/2005_2010.csv\"\n",
    "csv_2011_2017 = \"csv/2011_2017.csv\"\n",
    "coci_cits = \"csv/coci_cits.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm_pdftext(t):\n",
    "    t = re.sub(r\"(\\w{1})\\-\\s(\\w{1})\", r\"\\1\\2\", t)\n",
    "    return t\n",
    "\n",
    "def norm_data(x):\n",
    "    x = x.rstrip().lstrip()\n",
    "    regex = r\"(\\d{4})\"\n",
    "    matches = re.finditer(regex, x, re.MULTILINE)\n",
    "    for matchNum, match in enumerate(matches, start=1):\n",
    "        if match:\n",
    "            return match.group()\n",
    "    return \"none\"\n",
    "\n",
    "def norm_source(x):\n",
    "    x = x.rstrip().lstrip().lower()\n",
    "    if x == \"doi.org\":\n",
    "        return \"doi\"\n",
    "    if x == \"other\":\n",
    "        return \"other\"\n",
    "    return \"none\"\n",
    "    \n",
    "def norm_title(x):\n",
    "    x = x.rstrip().lstrip().lower()\n",
    "    x = norm_pdftext(x)\n",
    "    return x\n",
    "\n",
    "def norm_abstract(x):\n",
    "    x = x.rstrip().lstrip().lower()\n",
    "    x = norm_pdftext(x)\n",
    "    return x\n",
    "\n",
    "def norm_section(x, intext_cits = None):\n",
    "    x = x.rstrip().lstrip()\n",
    "    sections = list(filter(None,[item for item in x.split(\";;\")])) \n",
    "    sections = [item.split(\";\") for item in sections]\n",
    "    for i,item_val in enumerate(sections): \n",
    "        for p,part_val in enumerate(item_val): \n",
    "            sections[i][p] = part_val.rstrip().lstrip().lower()\n",
    "            if sections[i][p] == \"none\":\n",
    "                return [\"none\" for j in range(0,intext_cits)]\n",
    "    return sections\n",
    "\n",
    "def norm_cits_text(x):\n",
    "    x = x.rstrip().lstrip()\n",
    "    cits_text = [norm_pdftext(item.rstrip().lstrip().lower()) for item in x.split(\";;\")]\n",
    "    cits_text = list(filter(None, cits_text))\n",
    "    return cits_text\n",
    "\n",
    "def norm_cit_intent(x):\n",
    "    x = x.rstrip().lstrip()\n",
    "    cit_intent = [item.rstrip().lstrip().lower() for item in x.split(\";;\")]\n",
    "    cit_intent = list(filter(None, cit_intent))\n",
    "    return cit_intent\n",
    "\n",
    "def norm_sentiment(x):\n",
    "    x = x.rstrip().lstrip()\n",
    "    sentiment = [item.rstrip().lstrip().lower() for item in x.split(\";;\")]\n",
    "    sentiment = list(filter(None, sentiment))\n",
    "    return sentiment\n",
    "\n",
    "def norm_retraction_men(x):\n",
    "    x = x.rstrip().lstrip().lower()\n",
    "    x = x.replace(\";;\",\"\")\n",
    "    return x\n",
    "\n",
    "def norm_note(x):\n",
    "    x = x.rstrip().lstrip()\n",
    "    note = [item.rstrip().lstrip().lower() for item in x.split(\";;\")]\n",
    "    note = list(filter(None, note))\n",
    "    return note\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) In-text citations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data structures\n",
    "#---\n",
    "valid_docs = dict()\n",
    "err_docs = dict()\n",
    "#--- Testing\n",
    "test_dates = defaultdict(list)\n",
    "test_titles = defaultdict(list)\n",
    "test_sources = defaultdict(list)\n",
    "test_ret_men = defaultdict(list)\n",
    "\n",
    "with open(csv_2011_2017) as a_file:\n",
    "    csv_reader = csv.reader(a_file, delimiter=',')\n",
    "    # skip the headers \n",
    "    # 0.Date,\n",
    "    # 1.DOI\n",
    "    # 2.Source\n",
    "    # 3.Title\n",
    "    # 4.Abstract\n",
    "    # 5.Section\n",
    "    # 6.Citations to retracted article\n",
    "    # 7.Citing reasons,\n",
    "    # 8.Sentiment (negative/neutral/positive)\n",
    "    # 9.Mentions the article retraction,\n",
    "    # 10.Notes\n",
    "    \n",
    "    #skip the header\n",
    "    next(csv_reader, None)\n",
    "    \n",
    "    #iterate all the csv rows\n",
    "    for row in csv_reader:\n",
    "        \n",
    "        #Classify documents into Vlaid and Error \n",
    "        #---\n",
    "        cits_text = norm_cits_text(row[6])\n",
    "        if len(cits_text) == 0:\n",
    "            err_docs[row[1]] = row[10]\n",
    "        else:\n",
    "            doi = row[1]\n",
    "            valid_docs[doi] = dict()\n",
    "            valid_docs[doi][\"year\"] = norm_data(row[0])\n",
    "            valid_docs[doi][\"source\"] = norm_source(row[2])\n",
    "            valid_docs[doi][\"title\"] = norm_title(row[3])\n",
    "            valid_docs[doi][\"abstract\"] = norm_abstract(row[4])\n",
    "            valid_docs[doi][\"cits_text\"] = cits_text\n",
    "            valid_docs[doi][\"section\"] = norm_section(row[5], len(cits_text))\n",
    "            valid_docs[doi][\"cit_intent\"] = norm_cit_intent(row[7])\n",
    "            valid_docs[doi][\"sentiment\"] = norm_sentiment(row[8])\n",
    "            valid_docs[doi][\"retraction_mention\"] = norm_retraction_men(row[9])\n",
    "            valid_docs[doi][\"note\"] = norm_note(row[10])\n",
    "            \n",
    "            #Testing the csv values\n",
    "            #---\n",
    "            test_dates[norm_data(row[0])].append(doi)\n",
    "            test_sources[norm_source(row[2])].append(doi)\n",
    "            test_titles[norm_title(row[3])].append(doi)\n",
    "            test_ret_men[norm_retraction_men(row[9])].append(doi)\n",
    "            is_valid = (len(valid_docs[doi][\"cits_text\"]) == len(valid_docs[doi][\"section\"]) == len(valid_docs[doi][\"cit_intent\"]) == len(valid_docs[doi][\"sentiment\"]))  \n",
    "            if not is_valid:\n",
    "                print(doi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis with pandas "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Columns are the fields\n",
    "#Rows are the DOIs \n",
    "df = pd.DataFrame.from_dict(valid_docs).transpose()\n",
    "df[\"intext_cit\"] = list(zip(df[\"cits_text\"],df[\"cit_intent\"],df[\"sentiment\"],df[\"section\"]))\n",
    "sub_df = df[['year', 'source', 'title', 'abstract', 'retraction_mention', 'note', 'intext_cit']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyse the DataFrame\n",
    "df_cts_x_doc = df[\"section\"].apply(lambda x : len(x))\n",
    "\n",
    "MEAN_CITxDOC = df_cts_x_doc.mean()\n",
    "TOT_INTEXT_CIT = df_cts_x_doc.sum()\n",
    "TOT_DOC = df_cts_x_doc.count()\n",
    "DOI_DOCs = df[df[\"source\"] == \"doi\"][\"source\"].count()\n",
    "OTHER_DOCs = df[df[\"source\"] == \"other\"][\"source\"].count()\n",
    "\n",
    "#COCI CITS\n",
    "COCI_CITS_DICT = {}\n",
    "with open(coci_cits, mode='r') as csv_file:\n",
    "    csv_reader = csv.DictReader(csv_file)\n",
    "    for row in csv_reader:\n",
    "        COCI_CITS_DICT[row[\"citing\"]] = row\n",
    "COCI_CITS_DF = pd.DataFrame.from_dict(COCI_CITS_DICT).transpose()\n",
    "COCI_CITS_DF['creation'] = COCI_CITS_DF['creation'].apply(lambda x: x[0:4])\n",
    "CITS_SOURCES =  COCI_CITS_DF[['creation','source_title','source_id']]\n",
    "SOURCES_BY_YEAR = defaultdict(set)\n",
    "for row in CITS_SOURCES.itertuples():\n",
    "    SOURCES_BY_YEAR[row[1]].add((row[2],row[3]))\n",
    "\n",
    "RET_MEN = defaultdict(int)       \n",
    "for item in list(df[\"retraction_mention\"]):\n",
    "    RET_MEN[item] += 1\n",
    "\n",
    "SENTIMENT_COUNT = defaultdict(int)\n",
    "CIT_INTENT_COUNT = defaultdict(int)\n",
    "PATTERN = defaultdict(int)\n",
    "for doi, item in df.iterrows(): \n",
    "    cits_num = len(item[\"intext_cit\"][0])\n",
    "    for cit_index in range(0,cits_num):\n",
    "        cit_intent_val = item[\"intext_cit\"][1][cit_index]\n",
    "        cit_sentiment_val = item[\"intext_cit\"][2][cit_index]\n",
    "        \n",
    "        CIT_INTENT_COUNT[cit_intent_val] += 1\n",
    "        SENTIMENT_COUNT[cit_sentiment_val] += 1  \n",
    "        PATTERN[(cit_intent_val,cit_sentiment_val,item[\"retraction_mention\"])] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write sources on csv\n",
    "all_sources = []\n",
    "for k,v in SOURCES_BY_YEAR.items():\n",
    "    for item in v: \n",
    "        all_sources.append({\"year\": k, \"source_title\": item[0], \"source_id\": item[1]})\n",
    "write_list(all_sources, \"coci_sources.csv\", header= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+ Total number of documents:  337\n",
      "\n",
      "+ Source: \n",
      "      from the editor page: 225\n",
      "      from other sources: 112\n",
      "\n",
      "+ Total number of in-text reference pointers:  442\n",
      "\n",
      "+ Average number of in-text reference pointers per document:  1.311572700296736\n",
      "\n",
      "+ Documents which mention the retraction:  128\n",
      "\n",
      "+ In-text reference pointers: \n",
      "\n",
      "      sentiment count:\n",
      "       neutral  :  231\n",
      "       negative  :  209\n",
      "       positive  :  2\n",
      "      -----\n",
      "\n",
      "      intent count:\n",
      "       discusses  :  122\n",
      "       cites for information  :  45\n",
      "       cites as evidence  :  45\n",
      "       qualifies  :  44\n",
      "       credits  :  43\n",
      "       disputes  :  41\n",
      "       critiques  :  39\n",
      "       obtains background from  :  34\n",
      "       describes  :  23\n",
      "       includes excerpt from  :  4\n",
      "       uses data from  :  1\n",
      "       refutes  :  1\n",
      "      -----\n",
      "\n",
      "      common patters (intent, sentiment, mentions retraction):\n",
      "       ('discusses', 'negative', 'yes')  :  38\n",
      "       ('discusses', 'neutral', 'yes')  :  36\n",
      "       ('discusses', 'negative', 'no')  :  26\n",
      "       ('cites for information', 'neutral', 'no')  :  23\n",
      "       ('critiques', 'negative', 'no')  :  23\n",
      "       ('obtains background from', 'neutral', 'yes')  :  22\n",
      "       ('discusses', 'neutral', 'no')  :  20\n",
      "       ('cites as evidence', 'negative', 'no')  :  20\n",
      "       ('credits', 'neutral', 'no')  :  16\n",
      "       ('critiques', 'negative', 'yes')  :  15\n",
      "       ('credits', 'negative', 'no')  :  14\n",
      "       ('disputes', 'neutral', 'no')  :  14\n",
      "       ('qualifies', 'neutral', 'yes')  :  14\n",
      "       ('describes', 'neutral', 'no')  :  12\n",
      "       ('qualifies', 'negative', 'no')  :  12\n",
      "       ('disputes', 'neutral', 'yes')  :  11\n",
      "       ('cites for information', 'neutral', 'yes')  :  11\n",
      "       ('cites as evidence', 'negative', 'yes')  :  10\n",
      "       ('disputes', 'negative', 'no')  :  10\n",
      "       ('qualifies', 'negative', 'yes')  :  10\n",
      "       ('cites as evidence', 'neutral', 'no')  :  9\n",
      "       ('credits', 'neutral', 'yes')  :  9\n",
      "       ('qualifies', 'neutral', 'no')  :  8\n",
      "       ('describes', 'neutral', 'yes')  :  8\n",
      "       ('cites for information', 'negative', 'no')  :  7\n",
      "       ('disputes', 'negative', 'yes')  :  6\n",
      "       ('obtains background from', 'neutral', 'no')  :  6\n",
      "       ('cites as evidence', 'neutral', 'yes')  :  6\n",
      "       ('obtains background from', 'negative', 'yes')  :  6\n",
      "       ('includes excerpt from', 'neutral', 'yes')  :  4\n",
      "       ('cites for information', 'negative', 'yes')  :  4\n",
      "       ('credits', 'negative', 'yes')  :  4\n",
      "       ('discusses', 'positive', 'yes')  :  2\n",
      "       ('describes', 'negative', 'no')  :  2\n",
      "       ('critiques', 'neutral', 'no')  :  1\n",
      "       ('describes', 'negative', 'yes')  :  1\n",
      "       ('uses data from', 'neutral', 'no')  :  1\n",
      "       ('refutes', 'negative', 'yes')  :  1\n"
     ]
    }
   ],
   "source": [
    "print(\"+ Total number of documents: \",TOT_DOC)\n",
    "print(\"\\n+ Source: \")\n",
    "print(\"      from the editor page: \"+str(DOI_DOCs))\n",
    "print(\"      from other sources: \"+str(OTHER_DOCs))\n",
    "print(\"\\n+ Total number of in-text reference pointers: \",TOT_INTEXT_CIT)\n",
    "print(\"\\n+ Average number of in-text reference pointers per document: \",MEAN_CITxDOC)\n",
    "print(\"\\n+ Documents which mention the retraction: \", RET_MEN['yes'])\n",
    "print(\"\\n+ In-text reference pointers: \")\n",
    "print(\"\\n      sentiment count:\")\n",
    "\n",
    "SENTIMENT_COUNT = {k: v for k, v in sorted(dict(SENTIMENT_COUNT).items(), key=lambda item: item[1],reverse=True)}\n",
    "for k in SENTIMENT_COUNT:\n",
    "    print(\"      \",k,\" : \",SENTIMENT_COUNT[k] )\n",
    "print(\"      -----\")\n",
    "print(\"\\n      intent count:\")\n",
    "CIT_INTENT_COUNT = {k: v for k, v in sorted(dict(CIT_INTENT_COUNT).items(), key=lambda item: item[1],reverse=True)}\n",
    "for k in CIT_INTENT_COUNT:\n",
    "    print(\"      \",k,\" : \",CIT_INTENT_COUNT[k] )\n",
    "\n",
    "print(\"      -----\")\n",
    "print(\"\\n      common patters (intent, sentiment, mentions retraction):\")\n",
    "PATTERN = {k: v for k, v in sorted(dict(PATTERN).items(), key=lambda item: item[1],reverse=True)}\n",
    "for k in PATTERN:\n",
    "    print(\"      \",k,\" : \",PATTERN[k] )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
