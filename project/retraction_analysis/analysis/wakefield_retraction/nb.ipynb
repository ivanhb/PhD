{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import re\n",
    "import requests\n",
    "import os\n",
    "from datetime import datetime\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Usefull functions\n",
    "\n",
    "def write_list(l,file_path, header= True):\n",
    "    f = open(file_path,\"w+\")\n",
    "    initial_pos = 0\n",
    "    \n",
    "    #header\n",
    "    if header:\n",
    "        initial_pos = 1\n",
    "        str_header = ''\n",
    "        for k_header in l[0].keys():\n",
    "            str_header = str_header + str(k_header) + \",\"\n",
    "        f.write(str_header[:-1]+\"\\n\")\n",
    "        \n",
    "    #content\n",
    "    for l_index in range(initial_pos,len(l)):\n",
    "        str_row = ''\n",
    "        for k_att in l[l_index]:\n",
    "            str_row = str_row + '\"'+str(l[l_index][k_att]) +'\"'+','\n",
    "        f.write(str_row[:-1]+\"\\n\")\n",
    "        \n",
    "\n",
    "def coci_call(operation, list_dois, fields):\n",
    "    items_dict = {}\n",
    "    for doi in list_dois:\n",
    "        r = requests.get('https://opencitations.net/index/coci/api/v1/'+str(operation)+\"/\"+str(doi))\n",
    "        if len(r.json()) > 0: \n",
    "            if fields == \"*\":\n",
    "                items_dict[doi] = r.json()[0]\n",
    "            else:\n",
    "                items_dict[doi] = {}\n",
    "                for f in fields:\n",
    "                    items_dict[doi][f] = None\n",
    "                    if f in r.json()[0]:\n",
    "                        items_dict[doi][f] = r.json()[0][f]\n",
    "    return items_dict\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_1998_2004 = \"csv/1998_2004.csv\"\n",
    "csv_2005_2010 = \"csv/2005_2010.csv\"\n",
    "csv_2011_2017 = \"csv/2011_2017.csv\"\n",
    "coci_cits = \"csv/coci_cits.csv\"\n",
    "coci_sources = \"csv/coci_sources.csv\"\n",
    "coci_sources_isbn = \"csv/isbn/isbn_cat_lcc.csv\"\n",
    "lcc_scimago_map = \"csv/lcc_scimago_subject.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm_isbn_code(x):\n",
    "    regex = r\"^([A-Z]{1,})\\d\"\n",
    "    matches = re.finditer(regex, x, re.MULTILINE)\n",
    "    for matchNum, match in enumerate(matches, start=1):\n",
    "        if match:\n",
    "            return match.groups()[0]\n",
    "        \n",
    "def norm_pdftext(t):\n",
    "    t = re.sub(r\"(\\w{1})\\-\\s(\\w{1})\", r\"\\1\\2\", t)\n",
    "    return t\n",
    "\n",
    "def norm_data(x):\n",
    "    x = x.rstrip().lstrip()\n",
    "    regex = r\"(\\d{4})\"\n",
    "    matches = re.finditer(regex, x, re.MULTILINE)\n",
    "    for matchNum, match in enumerate(matches, start=1):\n",
    "        if match:\n",
    "            return match.group()\n",
    "    return \"none\"\n",
    "\n",
    "def norm_source(x):\n",
    "    x = x.rstrip().lstrip().lower()\n",
    "    if x == \"doi.org\":\n",
    "        return \"doi\"\n",
    "    if x == \"other\":\n",
    "        return \"other\"\n",
    "    return \"none\"\n",
    "    \n",
    "def norm_title(x):\n",
    "    x = x.rstrip().lstrip().lower()\n",
    "    x = norm_pdftext(x)\n",
    "    return x\n",
    "\n",
    "def norm_abstract(x):\n",
    "    x = x.rstrip().lstrip().lower()\n",
    "    x = norm_pdftext(x)\n",
    "    return x\n",
    "\n",
    "def norm_section(x, intext_cits = None):\n",
    "    x = x.rstrip().lstrip()\n",
    "    sections = list(filter(None,[item for item in x.split(\";;\")])) \n",
    "    sections = [item.split(\";\") for item in sections]\n",
    "    for i,item_val in enumerate(sections): \n",
    "        for p,part_val in enumerate(item_val): \n",
    "            sections[i][p] = part_val.rstrip().lstrip().lower()\n",
    "            if sections[i][p] == \"none\":\n",
    "                return [\"none\" for j in range(0,intext_cits)]\n",
    "    return sections\n",
    "\n",
    "def norm_cits_text(x):\n",
    "    x = x.rstrip().lstrip()\n",
    "    cits_text = [norm_pdftext(item.rstrip().lstrip().lower()) for item in x.split(\";;\")]\n",
    "    cits_text = list(filter(None, cits_text))\n",
    "    return cits_text\n",
    "\n",
    "def norm_cit_intent(x):\n",
    "    x = x.rstrip().lstrip()\n",
    "    cit_intent = [item.rstrip().lstrip().lower() for item in x.split(\";;\")]\n",
    "    cit_intent = list(filter(None, cit_intent))\n",
    "    return cit_intent\n",
    "\n",
    "def norm_sentiment(x):\n",
    "    x = x.rstrip().lstrip()\n",
    "    sentiment = [item.rstrip().lstrip().lower() for item in x.split(\";;\")]\n",
    "    sentiment = list(filter(None, sentiment))\n",
    "    return sentiment\n",
    "\n",
    "def norm_retraction_men(x):\n",
    "    x = x.rstrip().lstrip().lower()\n",
    "    x = x.replace(\";;\",\"\")\n",
    "    return x\n",
    "\n",
    "def norm_note(x):\n",
    "    x = x.rstrip().lstrip()\n",
    "    note = [item.rstrip().lstrip().lower() for item in x.split(\";;\")]\n",
    "    note = list(filter(None, note))\n",
    "    return note\n",
    "\n",
    "## Normalize sources\n",
    "\n",
    "def norm_subject(x):\n",
    "    x = x.rstrip().lstrip()\n",
    "    norm_val = [item.rstrip().lstrip().lower() for item in x.split(\";;\")]\n",
    "    norm_val = list(filter(None, norm_val))\n",
    "    return norm_val\n",
    "\n",
    "def norm_area(x, intext_cits = None):\n",
    "    x = x.rstrip().lstrip()\n",
    "    norm_val = list(filter(None,[item for item in x.split(\";;\")])) \n",
    "    norm_val = [item.split(\";\") for item in norm_val]   \n",
    "    for i,item_val in enumerate(norm_val): \n",
    "        for p,part_val in enumerate(item_val): \n",
    "            norm_val[i][p] = part_val.rstrip().lstrip().lower()\n",
    "    return norm_val\n",
    "\n",
    "def norm_source_id(x):\n",
    "    def filter_null(x):\n",
    "        return x[0] != \"\"\n",
    "    \n",
    "    x = x.rstrip().lstrip()\n",
    "    norm_val = [item.rstrip().lstrip().lower() for item in x.split(\";\")]\n",
    "    norm_val = [tuple(item.split(\":\")) for item in norm_val]\n",
    "    norm_val = list(filter(filter_null, norm_val))\n",
    "    return norm_val\n",
    "\n",
    "def norm_dois(x):\n",
    "    x = x.rstrip().lstrip()\n",
    "    norm_val = [item.rstrip().lstrip().lower() for item in x.split(\"[[;;]]\")]\n",
    "    norm_val = list(filter(None, norm_val))\n",
    "    return norm_val\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) In-text citations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data structures\n",
    "#---\n",
    "valid_docs = dict()\n",
    "err_docs = dict()\n",
    "#--- Testing\n",
    "test_dates = defaultdict(list)\n",
    "test_titles = defaultdict(list)\n",
    "test_sources = defaultdict(list)\n",
    "test_ret_men = defaultdict(list)\n",
    "\n",
    "with open(csv_2011_2017) as a_file:\n",
    "    csv_reader = csv.reader(a_file, delimiter=',')\n",
    "    # skip the headers \n",
    "    # 0.Date,\n",
    "    # 1.DOI\n",
    "    # 2.Source\n",
    "    # 3.Title\n",
    "    # 4.Abstract\n",
    "    # 5.Section\n",
    "    # 6.Citations to retracted article\n",
    "    # 7.Citing reasons,\n",
    "    # 8.Sentiment (negative/neutral/positive)\n",
    "    # 9.Mentions the article retraction,\n",
    "    # 10.Notes\n",
    "    \n",
    "    #skip the header\n",
    "    next(csv_reader, None)\n",
    "    \n",
    "    #iterate all the csv rows\n",
    "    for row in csv_reader:\n",
    "        \n",
    "        #Classify documents into Vlaid and Error \n",
    "        #---\n",
    "        cits_text = norm_cits_text(row[6])\n",
    "        if len(cits_text) == 0:\n",
    "            err_docs[row[1]] = row[10]\n",
    "        else:\n",
    "            doi = row[1]\n",
    "            valid_docs[doi] = dict()\n",
    "            valid_docs[doi][\"year\"] = norm_data(row[0])\n",
    "            valid_docs[doi][\"source\"] = norm_source(row[2])\n",
    "            valid_docs[doi][\"title\"] = norm_title(row[3])\n",
    "            valid_docs[doi][\"abstract\"] = norm_abstract(row[4])\n",
    "            valid_docs[doi][\"cits_text\"] = cits_text\n",
    "            valid_docs[doi][\"section\"] = norm_section(row[5], len(cits_text))\n",
    "            valid_docs[doi][\"cit_intent\"] = norm_cit_intent(row[7])\n",
    "            valid_docs[doi][\"sentiment\"] = norm_sentiment(row[8])\n",
    "            valid_docs[doi][\"retraction_mention\"] = norm_retraction_men(row[9])\n",
    "            valid_docs[doi][\"note\"] = norm_note(row[10])\n",
    "            \n",
    "            #Testing the csv values\n",
    "            #---\n",
    "            test_dates[norm_data(row[0])].append(doi)\n",
    "            test_sources[norm_source(row[2])].append(doi)\n",
    "            test_titles[norm_title(row[3])].append(doi)\n",
    "            test_ret_men[norm_retraction_men(row[9])].append(doi)\n",
    "            is_valid = (len(valid_docs[doi][\"cits_text\"]) == len(valid_docs[doi][\"section\"]) == len(valid_docs[doi][\"cit_intent\"]) == len(valid_docs[doi][\"sentiment\"]))  \n",
    "            if not is_valid:\n",
    "                print(doi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) All Citing sources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1) ISBNs Citations (update the sources dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "SUBJECTS_ISBN = defaultdict(str)\n",
    "LCC_SCIMAGO_MAP = defaultdict(str)\n",
    "with open(coci_sources_isbn) as a_file:\n",
    "    #0. ISBN\n",
    "    #1. CODE\n",
    "    csv_reader = csv.reader(a_file, delimiter=',') \n",
    "    for row in csv_reader:\n",
    "        SUBJECTS_ISBN[row[0]] = row[1]\n",
    "        \n",
    "with open(lcc_scimago_map) as a_file:\n",
    "    #0. LCC Code\n",
    "    #1. SCIMAGO Subject\n",
    "    csv_reader = csv.reader(a_file, delimiter=',') \n",
    "    for row in csv_reader:\n",
    "        LCC_SCIMAGO_MAP[row[0]] = row[1]\n",
    "\n",
    "# The Sources ISBN\n",
    "MY_ISBN_LIST = [[\"9780230282889\",\"9780230369078\"],[\"9780333922637\",\"9780230213999\"],[\"9780470710470\",\"9780470745328\"],[\"9780470918449\",\"9780470381120\"],[\"9780470939345\",\"9780471716969\"],[\"9780470939390\",\"9780471237372\"],[\"9780470939406\",\"9780471237389\"],[\"9780470976739\",\"9780470694671\"],[\"9780824705107\",\"9781420002515\"],[\"9780824707156\",\"9780824755164\"],[\"9780824750619\",\"9780203026229\"],[\"9781118404898\",\"9780470029718\"],[\"9781118517000\",\"9780470745915\"],[\"9781118543504\",\"9780470654675\"],[\"9781118683484\",\"9781119940418\"],[\"9781118688489\",\"9780813806143\"],[\"9781118753378\",\"9781118845479\"],[\"9781118858080\",\"9781118128336\"],[\"9781118898345\",\"9781118898390\"],[\"9781119164746\",\"9781119164777\"],[\"9781119426981\",\"9781118586624\"],[\"9781119943280\",\"9780470667347\"],[\"9781119959946\",\"9780470654750\"],[\"9781349318230\",\"9781137023001\"],[\"9781420060737\",\"9781420060744\"],[\"9781420068818\",\"9781420068870\"],[\"9781439804797\",\"9781439804827\"],[\"9781439813430\",\"9781439813447\"],[\"9781439838839\",\"9781439838846\"],[\"9781444325461\",\"9781405186544\"],[\"9781444355666\",\"9781444337082\"],[\"9781466567207\",\"9781466567238\"],[\"9781841845203\",\"9780203007648\"],[\"9781848550803\",\"9781848550810\"],[\"9783319159485\",\"9783319159492\"],[\"9783319180953\",\"9783319180960\"],[\"9783319283241\",\"9783319283265\"],[\"9783319309231\",\"9783319309255\"],[\"9783319311432\",\"9783319311432\"],[\"9783319418988\",\"9783319418995\"],[\"9783319432663\",\"9783319432687\"],[\"9783319599502\",\"9783319599526\"],[\"9783319625416\",\"9783319625430\"],[\"9783319638225\",\"9783319638232\"],[\"9783319652641\",\"9783319652665\"],[\"9783319669380\",\"9783319669397\"],[\"9783319693491\",\"9783319693507\"],[\"9783662495032\",\"9783662495049\"],[\"9783662547984\",\"9783662547991\"]]\n",
    "MY_ISBN_LCC = defaultdict(str)\n",
    "for a_book in MY_ISBN_LIST:\n",
    "    sub_val = \"none\"\n",
    "    for i_isbn in range(0,2):\n",
    "        if a_book[i_isbn] in SUBJECTS_ISBN: \n",
    "                sub_val = SUBJECTS_ISBN[a_book[i_isbn]]\n",
    "                break\n",
    "    MY_ISBN_LCC[tuple(a_book)] = sub_val\n",
    "# Others manually retrieved from https://catalog.loc.gov\n",
    "MY_ISBN_LCC[(\"9781420068818\",\"9781420068870\")] = \"RC553.A88\"\n",
    "\n",
    "for isbn,code in MY_ISBN_LCC.items():\n",
    "    if code != \"none\":\n",
    "        if code in LCC_SCIMAGO_MAP:\n",
    "            sub_val = LCC_SCIMAGO_MAP[code]\n",
    "        else:\n",
    "            sub_val = LCC_SCIMAGO_MAP[norm_isbn_code(code)]\n",
    "            father_code = norm_isbn_code(code)[0:-1]\n",
    "            while sub_val == \"\" and len(father_code)>0:\n",
    "                sub_val = LCC_SCIMAGO_MAP[father_code]\n",
    "                father_code = father_code[0:-1]\n",
    "        \n",
    "        MY_ISBN_SUBJECT_DICT[isbn] = sub_val\n",
    "        CSV_READY_ISBN_SUBJECTS.append({\"isbn\":\";\".join(isbn),\"lcc\":code,\"subject\":sub_val})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "UPDATED_COCI_SOURCES = []\n",
    "\n",
    "with open(coci_sources) as a_file:\n",
    "    csv_reader = csv.reader(a_file, delimiter=',') \n",
    "    #0. date\n",
    "    #1. subject\n",
    "    #2. area\n",
    "    #3. source_title\n",
    "    #4. source_id\n",
    "    #5. notes\n",
    "    #6. doi\n",
    "\n",
    "    #skip the header\n",
    "    next(csv_reader, None)\n",
    "    #iterate all the csv rows\n",
    "    for row in csv_reader:\n",
    "        updated_row = row\n",
    "        k = tuple(row[4].replace(\"isbn:\",\"\").split(\"; \"))\n",
    "        if k in MY_ISBN_SUBJECT_DICT:\n",
    "            updated_row[1] = MY_ISBN_SUBJECT_DICT[k]\n",
    "        dict_updated_row = dict()\n",
    "        for i in range(0,len(row)):\n",
    "            dict_updated_row[i] = row[i]\n",
    "        UPDATED_COCI_SOURCES.append(dict_updated_row)\n",
    "        \n",
    "write_list(UPDATED_COCI_SOURCES, \"coci_sources_updated.csv\", header= False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2) All Citing Sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_sources = []\n",
    "#--- Testing\n",
    "test_dates = defaultdict(list)\n",
    "test_titles = defaultdict(list)\n",
    "test_sources = defaultdict(list)\n",
    "test_ret_men = defaultdict(list)\n",
    "with open(coci_sources) as a_file:\n",
    "    csv_reader = csv.reader(a_file, delimiter=',') \n",
    "    #0. date\n",
    "    #1. subject\n",
    "    #2. area\n",
    "    #3. source_title\n",
    "    #4. source_id\n",
    "    #5. notes\n",
    "    #6. doi\n",
    "\n",
    "    #iterate all the csv rows \n",
    "    for row in csv_reader:\n",
    "        is_valid = len(norm_subject(row[1])) > 0\n",
    "        if is_valid:\n",
    "            elem = {\n",
    "                \"year\" : int(norm_data(row[0])),\n",
    "                \"subject\" : norm_subject(row[1]),\n",
    "                \"area\" : norm_area(row[2]),\n",
    "                \"source_title\" : norm_title(row[3]),\n",
    "                \"source_id\" : norm_source_id(row[4]),\n",
    "                \"doi\" : norm_dois(row[6])\n",
    "            }\n",
    "            valid_sources.append(elem)     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis with pandas "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Columns are the fields\n",
    "#Rows are the DOIs \n",
    "df = pd.DataFrame.from_dict(valid_docs).transpose()\n",
    "df[\"intext_cit\"] = list(zip(df[\"cits_text\"],df[\"cit_intent\"],df[\"sentiment\"],df[\"section\"]))\n",
    "sub_df = df[['year', 'source', 'title', 'abstract', 'retraction_mention', 'note', 'intext_cit']]\n",
    "\n",
    "#Sources\n",
    "df_sources = pd.DataFrame.from_dict(valid_sources)\n",
    "df_sources[\"doi\"] = df_sources[\"doi\"].apply(lambda x: len(x))\n",
    "df_sources = df_sources[(df_sources[\"year\"] >= 2011) & (df_sources[\"year\"] <= 2017)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyse the DataFrame\n",
    "df_cts_x_doc = df[\"section\"].apply(lambda x : len(x))\n",
    "\n",
    "MEAN_CITxDOC = df_cts_x_doc.mean()\n",
    "TOT_INTEXT_CIT = df_cts_x_doc.sum()\n",
    "TOT_DOC = df_cts_x_doc.count()\n",
    "DOI_DOCs = df[df[\"source\"] == \"doi\"][\"source\"].count()\n",
    "OTHER_DOCs = df[df[\"source\"] == \"other\"][\"source\"].count()\n",
    "\n",
    "#COCI CITS\n",
    "COCI_CITS_DICT = {}\n",
    "with open(coci_cits, mode='r') as csv_file:\n",
    "    csv_reader = csv.DictReader(csv_file)\n",
    "    for row in csv_reader:\n",
    "        COCI_CITS_DICT[row[\"citing\"]] = row\n",
    "COCI_CITS_DF = pd.DataFrame.from_dict(COCI_CITS_DICT).transpose()\n",
    "COCI_CITS_DF['creation'] = COCI_CITS_DF['creation'].apply(lambda x: x[0:4])\n",
    "CITS_SOURCES =  COCI_CITS_DF[['creation','source_title','source_id']]\n",
    "SOURCES_BY_YEAR = defaultdict(set)\n",
    "for row in CITS_SOURCES.itertuples():\n",
    "    SOURCES_BY_YEAR[row[1]].add((row[2],row[3]))\n",
    "\n",
    "RET_MEN = defaultdict(int)       \n",
    "for item in list(df[\"retraction_mention\"]):\n",
    "    RET_MEN[item] += 1\n",
    "\n",
    "SENTIMENT_COUNT = defaultdict(int)\n",
    "CIT_INTENT_COUNT = defaultdict(int)\n",
    "PATTERN = defaultdict(int)\n",
    "for doi, item in df.iterrows(): \n",
    "    cits_num = len(item[\"intext_cit\"][0])\n",
    "    for cit_index in range(0,cits_num):\n",
    "        cit_intent_val = item[\"intext_cit\"][1][cit_index]\n",
    "        cit_sentiment_val = item[\"intext_cit\"][2][cit_index]\n",
    "        \n",
    "        CIT_INTENT_COUNT[cit_intent_val] += 1\n",
    "        SENTIMENT_COUNT[cit_sentiment_val] += 1  \n",
    "        PATTERN[(cit_intent_val,cit_sentiment_val,item[\"retraction_mention\"])] += 1\n",
    "        \n",
    "        \n",
    "# The Sources ISSN\n",
    "SUBJECTS = defaultdict(dict)\n",
    "sub_area = list(zip(df_sources[\"subject\"], list(df_sources[\"area\"]), list(df_sources[\"doi\"])))\n",
    "for tupla in sub_area:\n",
    "    #in case of an ISBN\n",
    "    if len(tupla[1]) == 0:\n",
    "            if \"ISBN\" not in SUBJECTS[tupla[0][0]]:\n",
    "                SUBJECTS[tupla[0][0]][\"ISBN\"] = 0\n",
    "            SUBJECTS[tupla[0][0]][\"ISBN\"] += tupla[2]\n",
    "    else:\n",
    "        for sub_index in range(0,len(tupla[0])):\n",
    "            for area in tupla[1][sub_index]:\n",
    "                if area not in SUBJECTS[tupla[0][sub_index]]:\n",
    "                    SUBJECTS[tupla[0][sub_index]][area] = 0\n",
    "                SUBJECTS[tupla[0][sub_index]][area] += tupla[2]\n",
    "\n",
    "READY_TO_CSV = []\n",
    "for k,v in SUBJECTS.items():\n",
    "    for k_area in v:\n",
    "        READY_TO_CSV.append({\"subject\":k,\"area\":k_area,\"count\":v[k_area]})\n",
    "write_list(READY_TO_CSV, \"sources_test.csv\", header= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"+ Total number of documents: \",TOT_DOC)\n",
    "print(\"\\n+ Source: \")\n",
    "print(\"      from the editor page: \"+str(DOI_DOCs))\n",
    "print(\"      from other sources: \"+str(OTHER_DOCs))\n",
    "print(\"\\n+ Total number of in-text reference pointers: \",TOT_INTEXT_CIT)\n",
    "print(\"\\n+ Average number of in-text reference pointers per document: \",MEAN_CITxDOC)\n",
    "print(\"\\n+ Documents which mention the retraction: \", RET_MEN['yes'])\n",
    "print(\"\\n+ In-text reference pointers: \")\n",
    "print(\"\\n      sentiment count:\")\n",
    "\n",
    "SENTIMENT_COUNT = {k: v for k, v in sorted(dict(SENTIMENT_COUNT).items(), key=lambda item: item[1],reverse=True)}\n",
    "for k in SENTIMENT_COUNT:\n",
    "    print(\"      \",k,\" : \",SENTIMENT_COUNT[k] )\n",
    "print(\"      -----\")\n",
    "print(\"\\n      intent count:\")\n",
    "CIT_INTENT_COUNT = {k: v for k, v in sorted(dict(CIT_INTENT_COUNT).items(), key=lambda item: item[1],reverse=True)}\n",
    "for k in CIT_INTENT_COUNT:\n",
    "    print(\"      \",k,\" : \",CIT_INTENT_COUNT[k] )\n",
    "\n",
    "print(\"      -----\")\n",
    "print(\"\\n      common patters (intent, sentiment, mentions retraction):\")\n",
    "PATTERN = {k: v for k, v in sorted(dict(PATTERN).items(), key=lambda item: item[1],reverse=True)}\n",
    "for k in PATTERN:\n",
    "    print(\"      \",k,\" : \",PATTERN[k] )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
